{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "SRCNN_tunning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvvvmYIN-QCI"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-OL1E6VREl2"
      },
      "source": [
        "import os\n",
        "import easydict\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils import data as D\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3iRkchP-WUZ"
      },
      "source": [
        "# Mount google drive to colab environment\n",
        "The location will at /content/gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QVBj6-RfHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518f0276-da92-4fc9-def3-798d97261ee4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUgEbyo3-gn5"
      },
      "source": [
        "# Check mounted google drive and its contens\n",
        "Make sure you to save data inside google driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oEo2tENsjln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cebc43-efb7-47fa-8c77-f9e74bf86312"
      },
      "source": [
        "!pwd  #리눅스 현재 경로 명령어와 동일\n",
        "#!ls -l\n",
        "#!rm -rf /content/div2k_100/\n",
        "\n",
        "#파일 경로 /content/gdrive/MyDrive/srcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjZauhgbsxUU"
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/srcnn/div2k_100.zip\" -d \"/content/gdrive/My Drive/srcnn/div2k_100-2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEZ5gS-URsbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edfa7a1-1a8d-469e-bc67-e79840be0824"
      },
      "source": [
        "!ls -l \"/content/gdrive/My Drive/srcnn/div2k_100-2\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwx------ 2 root root 4096 Dec 23 09:13 checkpoint_dir\n",
            "drwx------ 4 root root 4096 Dec 23 09:19 test_images\n",
            "drwx------ 4 root root 4096 Dec 23 09:21 train_patches_x4lr64\n",
            "drwx------ 4 root root 4096 Dec 23 09:19 valid_patches_x4lr64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TODZIl3SqUPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32edf36-02d3-49b6-aa22-f231e621bfec"
      },
      "source": [
        "!python --version\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "1.10.0+cu111\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rz17z8-2wG"
      },
      "source": [
        "# Set your data directory into div2k_dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XYDDTynS-U7"
      },
      "source": [
        "div2k_dir = \"/content/gdrive/My Drive/srcnn/div2k_100-2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYlTmqZC-731"
      },
      "source": [
        "# Options to control overal programming behaviors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ocKQaztbREmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64623f06-bb51-4aec-991f-3e14108a4e78"
      },
      "source": [
        "# import argparse\n",
        "opt = easydict.EasyDict({\n",
        "    \"resume\": True,\n",
        "    \"resume_best\": True,\n",
        "    \"use_npy\": True,\n",
        "    \n",
        "    \"multi_gpu\": True,\n",
        "    \"use_cuda\": True,\n",
        "    \"device\": 'cuda',\n",
        "\n",
        "    \"n_epochs\": 100, # Total number of epoch to iterate\n",
        "    \"batch_size\": 100, # Size of batch of one epoch\n",
        "    \"start_epoch\": 1,\n",
        "    \"lr\": 1e-4, # Adam: learning rate\n",
        "    \"b1\": 0.9, # Adam: The exponential decay rate for the first moment estimates\n",
        "    \"b2\": 0.999, # Adam: The exponential decay rate for the second-moment estimates\n",
        "    \n",
        "    \"checkpoint_dir\": None,   # 학습 모델 저장 dir location\n",
        "    \"data_dir\": None,\n",
        "    \"train_dir\": None,\n",
        "    \"valid_dir\": None,\n",
        "    \"test_dir\": None,\n",
        "    \"test_result_dir\": None,\n",
        "    \n",
        "    \"lr_img\": 64,   # input patch size\n",
        "    \"res_scale\": 4, # output image 64 * 4 = 256\n",
        "    \"n_channels\": 3\n",
        "})\n",
        "\n",
        "opt.data_dir = div2k_dir\n",
        "opt.checkpoint_dir = os.path.join(opt.data_dir, \"checkpoint_dir\")\n",
        "opt.train_dir = os.path.join(opt.data_dir, \"train_patches_x\" + str(opt.res_scale) + \"lr\" + str(opt.lr_img))\n",
        "opt.valid_dir = os.path.join(opt.data_dir, \"valid_patches_x\" + str(opt.res_scale) + \"lr\" + str(opt.lr_img))\n",
        "opt.test_dir = os.path.join(opt.data_dir, \"test_images\")\n",
        "opt.test_result_dir = os.path.join(opt.data_dir, \"test_result\")\n",
        "\n",
        "print(opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'resume': True, 'resume_best': True, 'use_npy': True, 'multi_gpu': True, 'use_cuda': True, 'device': 'cuda', 'n_epochs': 100, 'batch_size': 100, 'start_epoch': 1, 'lr': 0.0001, 'b1': 0.9, 'b2': 0.999, 'checkpoint_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir', 'data_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2', 'train_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2/train_patches_x4lr64', 'valid_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2/valid_patches_x4lr64', 'test_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2/test_images', 'test_result_dir': '/content/gdrive/My Drive/srcnn/div2k_100-2/test_result', 'lr_img': 64, 'res_scale': 4, 'n_channels': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD3qZ8tZ_BvN"
      },
      "source": [
        "# Noramlization of image\n",
        "This function will make image range from 0 ~ 255 to 0 ~ 1.0 and convert type from int16 to float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np_O_GUgREmI"
      },
      "source": [
        "def normalize_img(img):\n",
        "    img = img / 255.\n",
        "    img = img.astype(np.float32)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aW_OTwF_WFu"
      },
      "source": [
        "# Define your dataset\n",
        "This defined dataset can be easily loaded by pytorch library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnamWOz2REmO"
      },
      "source": [
        "class DatasetFromFolder(data.Dataset):\n",
        "    def __init__(self, data_dir, use_npy):\n",
        "        super(DatasetFromFolder, self).__init__()\n",
        "\n",
        "        self.use_npy = use_npy\n",
        "\n",
        "        if self.use_npy:\n",
        "            lr_dir = os.path.join(data_dir, 'lr.npy')\n",
        "            hr_dir = os.path.join(data_dir, 'hr.npy')\n",
        "        else:\n",
        "            lr_dir = os.path.join(data_dir, 'lr')\n",
        "            hr_dir = os.path.join(data_dir, 'hr')\n",
        "        \n",
        "        self.dsets = {}\n",
        "\n",
        "        if self.use_npy:\n",
        "            self.dsets['lr'] = np.load(lr_dir)\n",
        "            self.dsets['hr'] = np.load(hr_dir)\n",
        "        else:\n",
        "            lr_list = os.listdir(lr_dir)\n",
        "            hr_list = os.listdir(hr_dir)\n",
        "\n",
        "            lr_list.sort()\n",
        "            hr_list.sort()\n",
        "            self.dsets['lr'] = [os.path.join(lr_dir, x) for x in lr_list]\n",
        "            self.dsets['hr'] = [os.path.join(hr_dir, x) for x in hr_list]\n",
        "        \n",
        "        self.dsets['file_name'] = os.listdir(os.path.join(data_dir, 'hr'))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.use_npy:\n",
        "            input = self.dsets['lr'][idx]\n",
        "            target = self.dsets['hr'][idx]\n",
        "        else:\n",
        "            input = cv2.imread(self.dsets['lr'][idx])\n",
        "            target = cv2.imread(self.dsets['hr'][idx])\n",
        "\n",
        "        file_name = self.dsets['file_name'][idx]\n",
        "\n",
        "        # 64 x 64 (width x height)-> 256 x 256 upscale before entering them into the network\n",
        "        #bicubic interpolation으로 input 전에 resize 해주기\n",
        "        input = cv2.resize(input, (target.shape[1], target.shape[0]), interpolation=cv2.INTER_CUBIC)  # normally ()pytorch convention (height, width)\n",
        "        \n",
        "        input = normalize_img(input)\n",
        "        target = normalize_img(target)\n",
        "\n",
        "        input = np.transpose(input, (2, 0, 1))  #  (height, width, channel) -> pytorch convention (channel, height, width)\n",
        "        target = np.transpose(target, (2, 0, 1))\n",
        "\n",
        "        # pytorch 용 tensor로 변환\n",
        "        input = torch.from_numpy(input).type(torch.FloatTensor)\n",
        "        target = torch.from_numpy(target).type(torch.FloatTensor)\n",
        "\n",
        "        return input, target, file_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dsets['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4j98DVs_d0-"
      },
      "source": [
        "# Shift function\n",
        "This function will shift your image from 0 ~ 1.0 to -mean ~ (1 - mean) for each RGB channel. This will make neural network to learn easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EvtKcwQREmU"
      },
      "source": [
        "# mean set to zero\n",
        "class MeanShift(nn.Conv2d):\n",
        "    def __init__(\n",
        "        self, rgb_range,\n",
        "        rgb_mean=(0.4488, 0.4371, 0.4040), rgb_std=(1.0, 1.0, 1.0), sign=-1):\n",
        "\n",
        "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
        "        std = torch.Tensor(rgb_std)\n",
        "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
        "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean) / std\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsRJiIc1_zvK"
      },
      "source": [
        "# Define your model\n",
        "See details of [paper](https://arxiv.org/abs/1501.00092)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2m-LcQx5REma"
      },
      "source": [
        "class SRCNN(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(SRCNN, self).__init__()\n",
        "        pix_range = 1.0\n",
        "        \n",
        "        self.sub_mean = MeanShift(pix_range)\n",
        "        self.add_mean = MeanShift(pix_range, sign=1)\n",
        "        \n",
        "        # CLASStorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
        "        #          padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "        self.conv1 = nn.Conv2d(opt.n_channels, 64, kernel_size=9, padding=4)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2) #filter size 9-5-5로 수정\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(32, opt.n_channels, kernel_size=5, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sub_mean(x)\n",
        "        #residual = x\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        #x = torch.add(x, residual)\n",
        "        \n",
        "        out = self.add_mean(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fiuLdCXAFcJ"
      },
      "source": [
        "# Prepare for your unwanted system or program down\n",
        "This function saves the progress of learning for each echo. You can load the mode later from the last learned model. Of course, you have to define the function of loading model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZfO7PpyREme"
      },
      "source": [
        "def save_checkpoint(srcnn, epoch, loss):\n",
        "    checkpoint_dir = os.path.abspath(opt.checkpoint_dir)\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"models_epoch_%04d_loss_%.6f.pth\" % (epoch, loss))\n",
        "    \n",
        "    if torch.cuda.device_count() > 1 and opt.multi_gpu:\n",
        "        state = {\"epoch\": epoch, \"srcnn\": srcnn.module}\n",
        "    else:\n",
        "        state = {\"epoch\": epoch, \"srcnn\": srcnn}\n",
        "\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print(\"Checkpoint saved to {}\".format(checkpoint_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeOIA41JREmi"
      },
      "source": [
        "def load_model(checkpoint_dir):\n",
        "    checkpoint_list = glob.glob(os.path.join(checkpoint_dir, \"*.pth\"))\n",
        "    checkpoint_list.sort()\n",
        "\n",
        "    if opt.resume_best:\n",
        "        loss_list = list(map(lambda x: float(os.path.basename(x).split('_')[4][:-4]), checkpoint_list))\n",
        "        best_loss_idx = loss_list.index(min(loss_list))\n",
        "        checkpoint_path = checkpoint_list[best_loss_idx]\n",
        "    else:\n",
        "        checkpoint_path = checkpoint_list[len(checkpoint_list) - 1]\n",
        "\n",
        "    srcnn = SRCNN(opt)\n",
        "\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        \n",
        "        n_epoch = checkpoint['epoch']\n",
        "        srcnn.load_state_dict(checkpoint['srcnn'].state_dict())\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                .format(checkpoint_path, n_epoch))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(checkpoint_path))\n",
        "        n_epoch = 0\n",
        "\n",
        "    return n_epoch + 1, srcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC-JBCYHAcvT"
      },
      "source": [
        "# Define your train step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvPI316BREmo"
      },
      "source": [
        "def train(opt, model, optimizer, data_loader, loss_criterion):\n",
        "    print(\"===> Training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_psnr = 0.0\n",
        "\n",
        "    for iteration, batch in enumerate(tqdm(data_loader), 1):\n",
        "        x, target = batch[0], batch[1]  # return input[0], target[1], file_name[2]\n",
        "        if opt.use_cuda:\n",
        "            x = x.to(opt.device)\n",
        "            target = target.to(opt.device)\n",
        "\n",
        "        out = model(x)    # SRCNN result\n",
        "\n",
        "        loss = loss_criterion(out, target)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        psnr = 10 * math.log10(1. / loss.item())\n",
        "        total_psnr += psnr\n",
        "\n",
        "        # print(\"Training %.2fs => Epoch[%d/%d](%d/%d): Loss: %.5f PSNR: %.5f\" %\n",
        "        #     (time.time() - start_time, opt.epoch_num, opt.n_epochs, iteration, len(data_loader), loss.item(), psnr))\n",
        "\n",
        "    total_loss = total_loss / iteration\n",
        "    total_psnr = total_psnr / iteration\n",
        "    \n",
        "    print(\"***Training %.2fs => Epoch[%d/%d]: Loss: %.5f PSNR: %.5f\" %\n",
        "        (time.time() - start_time, opt.epoch_num, opt.n_epochs, total_loss, total_psnr))\n",
        "\n",
        "    return (total_loss, total_psnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STG1ZpygAhey"
      },
      "source": [
        "# Define your evaluation method for validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYgyyOUiREms"
      },
      "source": [
        "def evaluate(opt, model, data_loader, loss_criterion):\n",
        "    print(\"===> Validation\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_psnr = 0.0\n",
        "    with torch.no_grad(): # no need to use backpropagation computation unlike training stage\n",
        "        for iteration, batch in enumerate(tqdm(data_loader), 1):\n",
        "            x, target = batch[0], batch[1]\n",
        "\n",
        "            if opt.use_cuda:\n",
        "                x = x.to(opt.device)\n",
        "                target = target.to(opt.device)\n",
        "\n",
        "            out = model(x)\n",
        "            \n",
        "            loss = loss_criterion(out, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            psnr = 10 * math.log10(1. / loss.item())\n",
        "            total_psnr += psnr\n",
        "\n",
        "            # print(\"Validation %.2fs => Epoch[%d/%d](%d/%d): Loss: %.5f PSNR: %.5f\" %\n",
        "            #     (time.time() - start_time, opt.epoch_num, opt.n_epochs, iteration, len(data_loader), loss.item(), psnr))\n",
        "\n",
        "    total_loss = total_loss / iteration\n",
        "    total_psnr = total_psnr / iteration\n",
        "    \n",
        "    print(\"***Validation %.2fs => Epoch[%d/%d]: Loss: %.5f PSNR: %.5f\" %\n",
        "        (time.time() - start_time, opt.epoch_num, opt.n_epochs, total_loss, total_psnr))\n",
        "\n",
        "    return (total_loss, total_psnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNvHIjzeAnjS"
      },
      "source": [
        "# Define whole training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWU1137oREm5"
      },
      "source": [
        "def run_train(opt, training_data_loader, validation_data_loader):\n",
        "    # Define what device we are using\n",
        "    if not os.path.exists(opt.checkpoint_dir):\n",
        "        os.makedirs(opt.checkpoint_dir)\n",
        "\n",
        "    log_file = os.path.join(opt.checkpoint_dir, \"srcnn_log.csv\")\n",
        "\n",
        "    print('[Initialize networks for training]')\n",
        "    srcnn = SRCNN(opt)\n",
        "    l2_criterion = nn.MSELoss()\n",
        "    print(srcnn)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if opt.resume:\n",
        "        opt.start_epoch, srcnn = load_model(opt.checkpoint_dir)\n",
        "    else: # csv file initialization\n",
        "        with open(log_file, mode='w') as f:\n",
        "            f.write(\"epoch,train_loss,valid_loss\\n\")\n",
        "\n",
        "    print(\"===> Setting GPU\")\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    if opt.use_cuda and torch.cuda.is_available():\n",
        "        opt.use_cuda = True\n",
        "        opt.device = 'cuda'\n",
        "    else:\n",
        "        opt.use_cuda = False\n",
        "        opt.device = 'cpu'\n",
        "        \n",
        "    if torch.cuda.device_count() > 1 and opt.multi_gpu:\n",
        "        print(\"Use \" + str(torch.cuda.device_count()) + \" GPUs\")\n",
        "        srcnn = nn.DataParallel(srcnn)\n",
        "\n",
        "    if opt.use_cuda:\n",
        "        srcnn = srcnn.to(opt.device)\n",
        "        l2_criterion = l2_criterion.to(opt.device)\n",
        "\n",
        "    print(\"===> Setting Optimizer\")\n",
        "    optimizer = torch.optim.Adam(srcnn.parameters(),  lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "    for epoch in range(opt.start_epoch, opt.n_epochs):\n",
        "        opt.epoch_num = epoch\n",
        "        train_loss, train_psnr = train(opt, srcnn, optimizer, training_data_loader, loss_criterion=l2_criterion)\n",
        "        valid_loss, valid_psnr = evaluate(opt, srcnn, validation_data_loader, loss_criterion=l2_criterion)\n",
        "\n",
        "        with open(log_file, mode='a') as f:\n",
        "            f.write(\"%d,%08f,%08f,%08f,%08f\\n\" % (\n",
        "                epoch,\n",
        "                train_loss,\n",
        "                train_psnr,\n",
        "                valid_loss,\n",
        "                valid_psnr\n",
        "            ))\n",
        "        save_checkpoint(srcnn, epoch, valid_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z-leTKronLpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlIg90EkAthx"
      },
      "source": [
        "# Now it's time to train the SRCNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "crr6jgVNREm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2800d422-c1b9-4bd3-8871-7acf87bd0f0a"
      },
      "source": [
        "train_dir = opt.train_dir\n",
        "valid_dir = opt.valid_dir\n",
        "print(\"train_dir is: {}\".format(train_dir))\n",
        "print(\"valid_dir is: {}\".format(valid_dir))\n",
        "\n",
        "target_size = (opt.lr_img * opt.res_scale, opt.lr_img * opt.res_scale)\n",
        "\n",
        "\n",
        "train_dataset = DatasetFromFolder(train_dir, opt.use_npy)\n",
        "valid_dataset = DatasetFromFolder(valid_dir, opt.use_npy)\n",
        "\n",
        "training_data_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=opt.batch_size,\n",
        "                                  shuffle=True)\n",
        "validation_data_loader = DataLoader(dataset=valid_dataset,\n",
        "                                    batch_size=opt.batch_size,\n",
        "                                    shuffle=False)\n",
        "\n",
        "run_train(opt, training_data_loader, validation_data_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dir is: /content/gdrive/My Drive/srcnn/div2k_100-2/train_patches_x4lr64\n",
            "valid_dir is: /content/gdrive/My Drive/srcnn/div2k_100-2/valid_patches_x4lr64\n",
            "[Initialize networks for training]\n",
            "SRCNN(\n",
            "  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "  (relu1): ReLU()\n",
            "  (conv2): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (relu2): ReLU()\n",
            "  (conv3): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            ")\n",
            "=> loading checkpoint '/content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0077_loss_0.002023.pth'\n",
            "=> loaded checkpoint '/content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0077_loss_0.002023.pth' (epoch 77)\n",
            "===> Setting GPU\n",
            "CUDA Available:  True\n",
            "===> Setting Optimizer\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.44s => Epoch[78/100]: Loss: 0.00323 PSNR: 25.02582\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.11s => Epoch[78/100]: Loss: 0.00206 PSNR: 27.57876\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0078_loss_0.002064.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.05s => Epoch[79/100]: Loss: 0.00284 PSNR: 25.51125\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.08s => Epoch[79/100]: Loss: 0.00202 PSNR: 27.68944\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0079_loss_0.002023.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.77s => Epoch[80/100]: Loss: 0.00278 PSNR: 25.63033\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.16s => Epoch[80/100]: Loss: 0.00202 PSNR: 27.69527\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0080_loss_0.002021.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.35s => Epoch[81/100]: Loss: 0.00283 PSNR: 25.52735\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.12s => Epoch[81/100]: Loss: 0.00202 PSNR: 27.69967\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0081_loss_0.002019.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.72s => Epoch[82/100]: Loss: 0.00280 PSNR: 25.56370\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.11s => Epoch[82/100]: Loss: 0.00202 PSNR: 27.69993\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0082_loss_0.002019.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.76s => Epoch[83/100]: Loss: 0.00280 PSNR: 25.56104\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.13s => Epoch[83/100]: Loss: 0.00202 PSNR: 27.70247\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0083_loss_0.002019.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.50s => Epoch[84/100]: Loss: 0.00279 PSNR: 25.57473\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.27s => Epoch[84/100]: Loss: 0.00202 PSNR: 27.70629\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0084_loss_0.002017.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:44<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 104.28s => Epoch[85/100]: Loss: 0.00279 PSNR: 25.59021\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.25s => Epoch[85/100]: Loss: 0.00202 PSNR: 27.70865\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0085_loss_0.002016.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:44<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 104.32s => Epoch[86/100]: Loss: 0.00283 PSNR: 25.53359\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.23s => Epoch[86/100]: Loss: 0.00202 PSNR: 27.70819\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0086_loss_0.002016.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.65s => Epoch[87/100]: Loss: 0.00277 PSNR: 25.64756\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.09s => Epoch[87/100]: Loss: 0.00201 PSNR: 27.71298\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0087_loss_0.002014.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.87s => Epoch[88/100]: Loss: 0.00282 PSNR: 25.53806\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.11s => Epoch[88/100]: Loss: 0.00201 PSNR: 27.71248\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0088_loss_0.002014.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.06s => Epoch[89/100]: Loss: 0.00279 PSNR: 25.56840\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.16s => Epoch[89/100]: Loss: 0.00202 PSNR: 27.70148\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0089_loss_0.002018.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.19s => Epoch[90/100]: Loss: 0.00276 PSNR: 25.69850\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.15s => Epoch[90/100]: Loss: 0.00201 PSNR: 27.71644\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0090_loss_0.002013.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.08s => Epoch[91/100]: Loss: 0.00278 PSNR: 25.60505\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.17s => Epoch[91/100]: Loss: 0.00201 PSNR: 27.70838\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0091_loss_0.002015.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.14s => Epoch[92/100]: Loss: 0.00284 PSNR: 25.52320\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.14s => Epoch[92/100]: Loss: 0.00202 PSNR: 27.70385\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0092_loss_0.002015.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.35s => Epoch[93/100]: Loss: 0.00282 PSNR: 25.55185\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.17s => Epoch[93/100]: Loss: 0.00201 PSNR: 27.71382\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0093_loss_0.002013.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.10s => Epoch[94/100]: Loss: 0.00281 PSNR: 25.54979\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.15s => Epoch[94/100]: Loss: 0.00203 PSNR: 27.66942\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0094_loss_0.002029.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.15s => Epoch[95/100]: Loss: 0.00281 PSNR: 25.53649\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.24s => Epoch[95/100]: Loss: 0.00201 PSNR: 27.72298\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0095_loss_0.002010.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.26s => Epoch[96/100]: Loss: 0.00282 PSNR: 25.54259\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.19s => Epoch[96/100]: Loss: 0.00201 PSNR: 27.73420\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0096_loss_0.002006.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.94s => Epoch[97/100]: Loss: 0.00275 PSNR: 25.70132\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.21s => Epoch[97/100]: Loss: 0.00203 PSNR: 27.67043\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0097_loss_0.002029.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:43<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 103.15s => Epoch[98/100]: Loss: 0.00279 PSNR: 25.58925\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.18s => Epoch[98/100]: Loss: 0.00200 PSNR: 27.73910\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0098_loss_0.002004.pth\n",
            "===> Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [01:42<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Training 102.86s => Epoch[99/100]: Loss: 0.00276 PSNR: 25.63481\n",
            "===> Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:08<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Validation 8.12s => Epoch[99/100]: Loss: 0.00200 PSNR: 27.74316\n",
            "Checkpoint saved to /content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0099_loss_0.002003.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgXgqBdTA0nk"
      },
      "source": [
        "# Test model\n",
        "We will use PSNR and SSIM metrics to measure how well the model is trained. We can skimage libraries to easily measure two metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEyL9DLUREnC"
      },
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhQQDJ0uBFyI"
      },
      "source": [
        "# Define test dataset\n",
        "This time, we will not use dataloader of pytorch function. Instead, we will load each image from test data directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1fod6xdREnG"
      },
      "source": [
        "def get_test_dataset(data_dir, res_scale):\n",
        "    lr_dir = os.path.join(data_dir, 'lrx' + str(res_scale))\n",
        "    hr_dir = os.path.join(data_dir, 'hr')\n",
        "        \n",
        "    dsets = {}\n",
        "    lr_list = os.listdir(lr_dir)\n",
        "    hr_list = os.listdir(hr_dir)\n",
        "    lr_list.sort()\n",
        "    hr_list.sort()\n",
        "    dsets['lr'] = [os.path.join(lr_dir, x) for x in lr_list]\n",
        "    dsets['hr'] = [os.path.join(hr_dir, x) for x in hr_list]\n",
        "    dsets['file_name'] = os.listdir(hr_dir)\n",
        "    \n",
        "    return dsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf97hI_RBRqB"
      },
      "source": [
        "# Define your test method of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOI83FfMREnK"
      },
      "source": [
        "def test_model(opt, test_dataset):\n",
        "    \n",
        "    lr_list = test_dataset['lr']\n",
        "    hr_list = test_dataset['hr']\n",
        "    filename_list = test_dataset['file_name']\n",
        "    \n",
        "    sr_compare_dir = os.path.join(opt.test_result_dir, \"compare\") #이미지 3개 비교가능(bicubic,prediction,정답)\n",
        "    sr_result_dir = os.path.join(opt.test_result_dir, \"sr\")\n",
        "    sr_input_dir = os.path.join(opt.test_result_dir, \"bc\") #bicubic 이미지 추출위한 폴더 생성\n",
        "\n",
        "    if not os.path.exists(sr_result_dir):\n",
        "        os.makedirs(sr_result_dir)\n",
        "    if not os.path.exists(sr_compare_dir):\n",
        "        os.makedirs(sr_compare_dir)\n",
        "    if not os.path.exists(sr_input_dir):\n",
        "        os.makedirs(sr_input_dir)    \n",
        "\n",
        "    opt.resume_best = True  # validation loss best\n",
        "    _, srcnn = load_model(opt.checkpoint_dir)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if torch.cuda.device_count() > 1 and opt.multi_gpu:\n",
        "        print(\"Use \" + str(torch.cuda.device_count()) + \" GPUs\")\n",
        "        srcnn = nn.DataParallel(srcnn)\n",
        "\n",
        "    if opt.use_cuda and torch.cuda.is_available():\n",
        "        opt.use_cuda = True\n",
        "        opt.device = 'cuda'\n",
        "    else:\n",
        "        opt.use_cuda = False\n",
        "        opt.device = 'cpu'\n",
        "\n",
        "    if opt.use_cuda:\n",
        "        srcnn = srcnn.to(opt.device)\n",
        "        criterion = criterion.to(opt.device)\n",
        "\n",
        "    hr_img_sz = (opt.lr_img * opt.res_scale, opt.lr_img * opt.res_scale)\n",
        "    result_img = np.zeros((hr_img_sz[0], hr_img_sz[1] * 3, opt.n_channels))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_num = 0\n",
        "        sum_bicubic_psnr = 0.\n",
        "        sum_sr_psnr = 0.\n",
        "        sum_bicubic_ssim = 0.\n",
        "        sum_sr_ssim = 0.\n",
        "        \n",
        "        avg_bicubic_psnr = 0.\n",
        "        avg_sr_psnr = 0.\n",
        "        avg_bicubic_ssim = 0.\n",
        "        avg_sr_ssim = 0.\n",
        "\n",
        "        bicubic_psnr_list = [] #표준편차 구하기 위한 리스트\n",
        "        sr_psnr_list = []                       \n",
        "        bicubic_ssim_list = []\n",
        "        sr_ssim_list = []\n",
        "\n",
        "        start_time = time.time()\n",
        "        for batch in zip(lr_list, hr_list, filename_list):\n",
        "            input_path, target_path, file_name = batch[0], batch[1], batch[2]\n",
        "\n",
        "            # print(input_path)\n",
        "            # print(target_path)\n",
        "\n",
        "            # train 과는 다르게 data_loader 쓰지 않고, patch 아닌 한장 한장이미지 전체를 불러옴.            \n",
        "            input = cv2.imread(input_path)\n",
        "            target = cv2.imread(target_path)\n",
        "            \n",
        "            input = cv2.resize(input, (target.shape[1], target.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "            \n",
        "            input = normalize_img(input)\n",
        "            target = normalize_img(target)\n",
        "            \n",
        "            input = np.transpose(input, (2, 0, 1))\n",
        "            target = np.transpose(target, (2, 0, 1))\n",
        "            \n",
        "            # 하나의 이미지 입력 (1)\n",
        "            input = input.reshape(1, input.shape[0], input.shape[1], input.shape[2])\n",
        "            target = target.reshape(1, target.shape[0], target.shape[1], target.shape[2])\n",
        "            \n",
        "            input = torch.from_numpy(input).type(torch.FloatTensor)\n",
        "            target = torch.from_numpy(target).type(torch.FloatTensor)\n",
        "\n",
        "            if opt.use_cuda:\n",
        "                input = input.to(opt.device)\n",
        "#                 target = target.to(opt.device)\n",
        "\n",
        "            out = srcnn(input)\n",
        "            \n",
        "            for i in range(input.size(0)): \n",
        "                if opt.use_cuda:\n",
        "                    input_arr = input[i].detach().to('cpu').data.numpy()\n",
        "                    sr_arr = out[i].detach().to('cpu').data.numpy()\n",
        "                else:\n",
        "                    input_arr = input[i].data.numpy() # float tensor to numpy\n",
        "                    sr_arr = out[i].data.numpy()\n",
        "                \n",
        "                target_arr = target[i].detach().data.numpy()\n",
        "        \n",
        "                input_arr = np.transpose(input_arr, (1, 2, 0))\n",
        "                sr_arr = np.transpose(sr_arr, (1, 2, 0))\n",
        "                target_arr = np.transpose(target_arr, (1, 2, 0))\n",
        "\n",
        "                # 강제적으로 결과 범주를 정해줌 (negative pixel 값도 도출됨?)                \n",
        "                sr_arr[sr_arr < 0.] = 0\n",
        "                sr_arr[sr_arr > 1.] = 1.\n",
        "                \n",
        "                # bicubic, sr result, GT reference for comparison purpose\n",
        "                compare_img = np.concatenate((input_arr, sr_arr, target_arr), axis=1)\n",
        "            \n",
        "                bicubic_psnr = psnr(input_arr, target_arr)\n",
        "                sr_psnr = psnr(sr_arr, target_arr)\n",
        "                \n",
        "                sum_bicubic_psnr += bicubic_psnr\n",
        "                bicubic_psnr_list.append(bicubic_psnr)\n",
        "                sum_sr_psnr += sr_psnr\n",
        "                sr_psnr_list.append(sr_psnr)\n",
        "                \n",
        "                bicubic_ssim = ssim(input_arr, target_arr, multichannel=True)\n",
        "                sr_ssim = ssim(sr_arr, target_arr, multichannel=True)\n",
        "\n",
        "                sum_bicubic_ssim += bicubic_ssim\n",
        "                bicubic_ssim_list.append(bicubic_ssim)\n",
        "                sum_sr_ssim += sr_ssim\n",
        "                sr_ssim_list.append(sr_ssim)\n",
        "                \n",
        "                compare_img = compare_img * 255 # [0 - 1] * 255\n",
        "                compare_img = compare_img.astype(np.int16)  # converst to float data type\n",
        "                sr_arr = sr_arr * 255\n",
        "                sr_arr = sr_arr.astype(np.int16)\n",
        "\n",
        "                input_arr = input_arr * 255\n",
        "                input_arr = input_arr.astype(np.int16)\n",
        "\n",
        "                cv2.imwrite(os.path.join(sr_compare_dir, file_name), compare_img)\n",
        "                cv2.imwrite(os.path.join(sr_result_dir, file_name), sr_arr)\n",
        "                cv2.imwrite(os.path.join(sr_input_dir, file_name), input_arr)\n",
        "                \n",
        "                print(file_name)\n",
        "                print(\"Bicubic PSNR: {:.8f}, Bicubic SSIM: {:.8f}, SR PSNR: {:.8f}, SR SSIM: {:.8f}\".format(\n",
        "                    bicubic_psnr, bicubic_ssim, sr_psnr, sr_ssim))\n",
        "                total_num += 1\n",
        "\n",
        "        avg_bicubic_psnr = sum_bicubic_psnr / total_num\n",
        "        std_bicubic_psnr = np.std(bicubic_psnr_list) #표준편차 구하기\n",
        "        avg_sr_psnr = sum_sr_psnr / total_num\n",
        "        std_sr_psnr = np.std(sr_psnr_list)\n",
        "\n",
        "        avg_bicubic_ssim = sum_bicubic_ssim / total_num\n",
        "        std_bicubic_ssim = np.std(bicubic_ssim_list)\n",
        "        avg_sr_ssim = sum_sr_ssim / total_num\n",
        "        std_sr_ssim = np.std(sr_ssim_list)\n",
        "\n",
        "        print(\"Time: {:.2f}\".format(time.time() - start_time))\n",
        "        print(\"Bicubic PSNR: {:.8f}\".format(avg_bicubic_psnr))\n",
        "        print(\"Bicubic PSNR STD: {:.8f}\".format(std_bicubic_psnr))\n",
        "        print(\"Bicubic SSIM: {:.8f}\".format(avg_bicubic_ssim))\n",
        "        print(\"Bicubic SSIM STD: {:.8f}\".format(std_bicubic_ssim))\n",
        "\n",
        "        print(\"SR PSNR: {:.8f}\".format(avg_sr_psnr))\n",
        "        print(\"SR PSNR STD: {:.8f}\".format(std_sr_psnr))\n",
        "        print(\"SR SSIM: {:.8f}\".format(avg_sr_ssim))\n",
        "        print(\"SR SSIM STD: {:.8f}\".format(std_sr_ssim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOcoSLEdREnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9449aff9-9a2b-4ecd-e20a-7722a5af4b96"
      },
      "source": [
        "test_dir = opt.test_dir\n",
        "res_scale = opt.res_scale\n",
        "print(\"test_dir is: {}\".format(test_dir))\n",
        "print(\"test_result_dir is: {}\".format(opt.test_result_dir))\n",
        "\n",
        "test_dataset = get_test_dataset(test_dir, res_scale)\n",
        "\n",
        "test_model(opt, test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_dir is: /content/gdrive/My Drive/srcnn/div2k_100-2/test_images\n",
            "test_result_dir is: /content/gdrive/My Drive/srcnn/div2k_100-2/test_result\n",
            "=> loading checkpoint '/content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0099_loss_0.002003.pth'\n",
            "=> loaded checkpoint '/content/gdrive/My Drive/srcnn/div2k_100-2/checkpoint_dir/models_epoch_0099_loss_0.002003.pth' (epoch 99)\n",
            "0830.png\n",
            "Bicubic PSNR: 23.65051715, Bicubic SSIM: 0.77931095, SR PSNR: 23.99733887, SR SSIM: 0.79907723\n",
            "0883.png\n",
            "Bicubic PSNR: 25.04664727, Bicubic SSIM: 0.79283191, SR PSNR: 25.54673404, SR SSIM: 0.82017736\n",
            "0884.png\n",
            "Bicubic PSNR: 24.20699310, Bicubic SSIM: 0.77717060, SR PSNR: 24.65827121, SR SSIM: 0.79984994\n",
            "0886.png\n",
            "Bicubic PSNR: 33.58014167, Bicubic SSIM: 0.96164323, SR PSNR: 34.23868006, SR SSIM: 0.96358766\n",
            "0891.png\n",
            "Bicubic PSNR: 23.90350388, Bicubic SSIM: 0.81219866, SR PSNR: 24.48856528, SR SSIM: 0.83068392\n",
            "Time: 24.21\n",
            "Bicubic PSNR: 26.07756061\n",
            "Bicubic PSNR STD: 3.78067693\n",
            "Bicubic SSIM: 0.82463107\n",
            "Bicubic SSIM STD: 0.06963403\n",
            "SR PSNR: 26.58591789\n",
            "SR PSNR STD: 3.85901860\n",
            "SR SSIM: 0.84267522\n",
            "SR SSIM STD: 0.06165152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ab3N0wG0oYUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}