{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw2_1970030_parkguena.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"hEUZ3qLEWtyg"},"source":["import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSiGUFdkaS38"},"source":["mnist_transform=transforms.Compose([\n","                              transforms.Resize((28,28)),\n","                              transforms.ToTensor(), # first, convert image to PyTorch (float) tensor\n","                              transforms.Normalize((0.5,), (0.5,)) # normalize inputs, x' = (x-u)/std, x'=[-1, 1] \n","                              ])     \n","\n","train_dataset = datasets.MNIST(root='./mnist_data', \n","                                  train=True, \n","                                  download=True, \n","                                  transform=mnist_transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True) \n","# shuffle training data before each epoch to avoid local min\n","\n","# validation or test \n","validation_dataset = datasets.MNIST(root='./mnist_data', \n","                                  train=False, \n","                                  download=True, \n","                                  transform=mnist_transform)\n","\n","validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=100, shuffle=False) \n","#print(train_dataset)\n","#print(validation_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zbBbsxjegop"},"source":["# contruct model architecture"]},{"cell_type":"code","metadata":{"id":"L8hJOKWdef0h"},"source":["class NN_MNIST(nn.Module):\n","  def __init__(self, NN_in, H1,H2,H3,NN_out):\n","    super().__init__()\n","    self.linear1 = nn.Linear(NN_in, H1)\n","    self.linear2 = nn.Linear(H1, H2)\n","    self.linear3 = nn.Linear(H2, H3)\n","    self.linear4 = nn.Linear(H3, NN_out)\n","\n","    nn.init.xavier_uniform_(self.linear1.weight)\n","    nn.init.xavier_uniform_(self.linear2.weight)\n","    nn.init.xavier_uniform_(self.linear3.weight)\n","    nn.init.xavier_uniform_(self.linear4.weight)\n","\n","  def forward(self, x):\n","    x = F.relu(self.linear1(x))\n","    x = F.relu(self.linear2(x))\n","    x = F.relu(self.linear3(x))\n","    x = self.linear4(x) # activation not needed for multi-class classification task\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cKUaNDY8g8ry"},"source":["# make model instance \n"]},{"cell_type":"code","metadata":{"id":"k-3h4gCGgfs4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605956774881,"user_tz":-540,"elapsed":660,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}},"outputId":"b440725b-f9c3-490c-82b6-704b0c66ba98"},"source":["torch.manual_seed(1)\n","model = NN_MNIST(784,512,256,128,10)\n","model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_MNIST(\n","  (linear1): Linear(in_features=784, out_features=512, bias=True)\n","  (linear2): Linear(in_features=512, out_features=256, bias=True)\n","  (linear3): Linear(in_features=256, out_features=128, bias=True)\n","  (linear4): Linear(in_features=128, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"Lc1nU9M8g_Me"},"source":["# optimizer "]},{"cell_type":"code","metadata":{"id":"nODj6oD3hDyv"},"source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n","criterion = nn.CrossEntropyLoss() # cf. BCE()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhmMC4Vlmsx0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605957578049,"user_tz":-540,"elapsed":788902,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}},"outputId":"0413262f-bcd2-493e-80c1-ea919dce87bc"},"source":["epochs = 40\n","running_loss_history = []\n","running_corrects_history = []\n","validation_running_loss_history = []\n","validation_running_corrects_history = []\n","\n","for eno in range(epochs):\n","\n","  running_loss = 0.0\n","  running_corrects = 0.0\n","  validation_running_loss = 0.0\n","  validation_running_corrects = 0.0\n","\n","  # a batch of 100 images x 600 batches -> 60,000 for a single epoch \n","  for inputs, labels in train_loader:     \n","    # input [100(batch size), 1(channel), 28(width), 28(height)] =>     \n","    inputs = inputs.view(inputs.shape[0], -1) # [100, 784]\n","    outputs = model(inputs) # logits (scores)\n","    loss = criterion(outputs, labels) # outputs [100, 1], labels [100]\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    #max function output(max, max_indices)\n","    _, pred_idx = torch.max(outputs, 1) # 2nd argument - the dim to reduce    \n","\n","    running_loss += loss.item() # loss per batch \n","    running_corrects += torch.sum(pred_idx == labels.data)\n","  else:\n","    with torch.no_grad():\n","      for validation_inputs, validation_labels in validation_loader:     \n","        # input [100(batch size), 1(channel), 28(width), 28(height)] =>     \n","        validation_inputs = validation_inputs.view(validation_inputs.shape[0], -1) # [100, 784]\n","        validation_outputs = model(validation_inputs) # logits (scores)\n","        validation_loss = criterion(validation_outputs, validation_labels) # outputs [100, 1], labels [100]\n","        \n","        #max function output(max, max_indices)\n","        _, pred_idx = torch.max(validation_outputs, 1) # 2nd argument - the dim to reduce    \n","\n","        validation_running_loss += validation_loss.item() # loss per batch \n","        validation_running_corrects += torch.sum(pred_idx == validation_labels.data)\n","\n","    epoch_loss = running_loss/len(train_loader) \n","    running_loss_history.append(epoch_loss)\n","    epoch_acc = (running_corrects.float()/inputs.shape[0]*100)/len(train_loader) \n","    running_corrects_history.append(epoch_acc)\n","    print('--------------- epoch: ', (eno+1), '---------------')\n","    print('training loss: {:.4f}, accuracy {:.4f}'.format(epoch_loss, epoch_acc.item()))   \n","\n","    validation_epoch_loss = validation_running_loss/len(validation_loader) \n","    validation_running_loss_history.append(validation_epoch_loss)\n","    validation_epoch_acc = (validation_running_corrects.float()/validation_inputs.shape[0]*100)/len(validation_loader) \n","    validation_running_corrects_history.append(validation_epoch_acc)    \n","    print('validation loss: {:.4f}, accuracy {:.4f}'.format(validation_epoch_loss, validation_epoch_acc.item())) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["--------------- epoch:  1 ---------------\n","training loss: 0.4116, accuracy 87.1667\n","validation loss: 0.2225, accuracy 93.2400\n","--------------- epoch:  2 ---------------\n","training loss: 0.1713, accuracy 94.8233\n","validation loss: 0.1256, accuracy 96.2800\n","--------------- epoch:  3 ---------------\n","training loss: 0.1234, accuracy 96.1750\n","validation loss: 0.1132, accuracy 96.5900\n","--------------- epoch:  4 ---------------\n","training loss: 0.0959, accuracy 97.0117\n","validation loss: 0.1134, accuracy 96.4900\n","--------------- epoch:  5 ---------------\n","training loss: 0.0801, accuracy 97.6050\n","validation loss: 0.1031, accuracy 96.5200\n","--------------- epoch:  6 ---------------\n","training loss: 0.0665, accuracy 97.9433\n","validation loss: 0.0830, accuracy 97.4000\n","--------------- epoch:  7 ---------------\n","training loss: 0.0571, accuracy 98.2933\n","validation loss: 0.0853, accuracy 97.2300\n","--------------- epoch:  8 ---------------\n","training loss: 0.0485, accuracy 98.5117\n","validation loss: 0.0846, accuracy 97.3900\n","--------------- epoch:  9 ---------------\n","training loss: 0.0423, accuracy 98.6767\n","validation loss: 0.0724, accuracy 97.6200\n","--------------- epoch:  10 ---------------\n","training loss: 0.0355, accuracy 98.9433\n","validation loss: 0.0723, accuracy 97.8700\n","--------------- epoch:  11 ---------------\n","training loss: 0.0306, accuracy 99.1167\n","validation loss: 0.0703, accuracy 97.8600\n","--------------- epoch:  12 ---------------\n","training loss: 0.0267, accuracy 99.2233\n","validation loss: 0.0867, accuracy 97.4300\n","--------------- epoch:  13 ---------------\n","training loss: 0.0229, accuracy 99.3400\n","validation loss: 0.0746, accuracy 97.7600\n","--------------- epoch:  14 ---------------\n","training loss: 0.0183, accuracy 99.5417\n","validation loss: 0.0714, accuracy 97.9100\n","--------------- epoch:  15 ---------------\n","training loss: 0.0162, accuracy 99.5967\n","validation loss: 0.0714, accuracy 97.9100\n","--------------- epoch:  16 ---------------\n","training loss: 0.0136, accuracy 99.6783\n","validation loss: 0.0759, accuracy 97.7200\n","--------------- epoch:  17 ---------------\n","training loss: 0.0121, accuracy 99.7067\n","validation loss: 0.0669, accuracy 98.0200\n","--------------- epoch:  18 ---------------\n","training loss: 0.0098, accuracy 99.7900\n","validation loss: 0.0672, accuracy 98.0700\n","--------------- epoch:  19 ---------------\n","training loss: 0.0083, accuracy 99.8400\n","validation loss: 0.0707, accuracy 97.9400\n","--------------- epoch:  20 ---------------\n","training loss: 0.0075, accuracy 99.8533\n","validation loss: 0.0700, accuracy 98.0300\n","--------------- epoch:  21 ---------------\n","training loss: 0.0059, accuracy 99.9167\n","validation loss: 0.0644, accuracy 98.1500\n","--------------- epoch:  22 ---------------\n","training loss: 0.0050, accuracy 99.9200\n","validation loss: 0.0703, accuracy 97.9700\n","--------------- epoch:  23 ---------------\n","training loss: 0.0042, accuracy 99.9533\n","validation loss: 0.0697, accuracy 98.1300\n","--------------- epoch:  24 ---------------\n","training loss: 0.0032, accuracy 99.9750\n","validation loss: 0.0697, accuracy 98.0900\n","--------------- epoch:  25 ---------------\n","training loss: 0.0028, accuracy 99.9750\n","validation loss: 0.0688, accuracy 98.0600\n","--------------- epoch:  26 ---------------\n","training loss: 0.0024, accuracy 99.9800\n","validation loss: 0.0709, accuracy 98.1200\n","--------------- epoch:  27 ---------------\n","training loss: 0.0023, accuracy 99.9850\n","validation loss: 0.0720, accuracy 98.1200\n","--------------- epoch:  28 ---------------\n","training loss: 0.0019, accuracy 99.9900\n","validation loss: 0.0708, accuracy 98.1200\n","--------------- epoch:  29 ---------------\n","training loss: 0.0016, accuracy 99.9967\n","validation loss: 0.0735, accuracy 98.1300\n","--------------- epoch:  30 ---------------\n","training loss: 0.0015, accuracy 99.9950\n","validation loss: 0.0712, accuracy 98.1500\n","--------------- epoch:  31 ---------------\n","training loss: 0.0014, accuracy 99.9933\n","validation loss: 0.0731, accuracy 98.1200\n","--------------- epoch:  32 ---------------\n","training loss: 0.0012, accuracy 99.9967\n","validation loss: 0.0740, accuracy 98.1300\n","--------------- epoch:  33 ---------------\n","training loss: 0.0011, accuracy 99.9983\n","validation loss: 0.0738, accuracy 98.1000\n","--------------- epoch:  34 ---------------\n","training loss: 0.0010, accuracy 99.9950\n","validation loss: 0.0780, accuracy 98.1100\n","--------------- epoch:  35 ---------------\n","training loss: 0.0010, accuracy 99.9983\n","validation loss: 0.0745, accuracy 98.1600\n","--------------- epoch:  36 ---------------\n","training loss: 0.0009, accuracy 100.0000\n","validation loss: 0.0755, accuracy 98.1500\n","--------------- epoch:  37 ---------------\n","training loss: 0.0008, accuracy 100.0000\n","validation loss: 0.0760, accuracy 98.1500\n","--------------- epoch:  38 ---------------\n","training loss: 0.0008, accuracy 100.0000\n","validation loss: 0.0753, accuracy 98.1500\n","--------------- epoch:  39 ---------------\n","training loss: 0.0007, accuracy 100.0000\n","validation loss: 0.0768, accuracy 98.1400\n","--------------- epoch:  40 ---------------\n","training loss: 0.0007, accuracy 100.0000\n","validation loss: 0.0767, accuracy 98.2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kuBHXUbqpD06"},"source":["# minist data visualization "]},{"cell_type":"code","metadata":{"id":"tZyKbTmYpDD9"},"source":["# iter() returns an iterator for the given iterable object\n","dataiter = iter(validation_loader)\n","inputs, labels = dataiter.next()\n","outputs = model(inputs.view(inputs.shape[0], -1)) # logits (scores)\n","_, pred_idx = torch.max(outputs, 1) # 2nd argument - the dim to reduce    \n","\n","fig = plt.figure(figsize=(25, 25))\n","\n","for idx in np.arange(len(dataiter)): \n","  ax = fig.add_subplot(10, 10, idx+1, xticks=[], yticks=[])\n","\n","  image = inputs[idx].clone().detach().numpy() # [1, 28, 28] = [channel, width, height]\n","  image = image.transpose(1, 2, 0) # [28, 28, 1]\n","  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5)) # denomalization : x' = (x-u)/std => x = x'*std + u\n","  image = image.clip(0, 1)\n","  plt.imshow(image)\n","\n","  ax.set_title(\"P{} / L{}\".format(str(pred_idx[idx].item()), str(labels[idx].item())), color=(\"red\" if pred_idx[idx]!=labels[idx] else \"black\"))\n","\n","\n","\n","  \n"],"execution_count":null,"outputs":[]}]}